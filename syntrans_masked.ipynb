{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Select number of threads to use\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"48\" # export OMP_NUM_THREADS=1\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"48\" # export OPENBLAS_NUM_THREADS=1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"48\" # export MKL_NUM_THREADS=1\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"48\" # export VECLIB_MAXIMUM_THREADS=1\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"48\" # export NUMEXPR_NUM_THREADS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID = os.getpid()\n",
    "PGID = os.getpgid(PID)\n",
    "print(f\"PID: {PID}, PGID: {PGID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['train'] = False\n",
    "config['epochs'] = 5\n",
    "config['bert'] = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/original/ud/UD_English_EWT/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(config['bert'])\n",
    "model = BertForMaskedLM.from_pretrained(config['bert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMaskedInputs(inputs):\n",
    "    \"\"\"\n",
    "    creates masked input embeddings and labels from tokenized text\n",
    "\n",
    "    :param inputs: tokenized text\n",
    "    :return: masked input embeddings and new column labels \n",
    "    \"\"\" \n",
    "    # Clone input ids (tokens) to create labels\n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    # create mask array with 15% masked tokens\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "        (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "    # Select indices of each nonzero (= selected) value as token to be masked\n",
    "    selection = []\n",
    "\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    # Mask selected tokens: replace with [MASK] code 103 in tensor\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntransDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for ud_file in glob.iglob(data_path + '**/*-train.txt', recursive=True):\n",
    "\n",
    "  ud_file = os.path.abspath(ud_file)\n",
    "  filename = os.path.basename(ud_file)\n",
    "  print(filename, flush = True)\n",
    "  # Load test data\n",
    "  with open(ud_file, 'r') as fp:\n",
    "    text.append(fp.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "inputs = createMaskedInputs(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SyntransDataset(inputs)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)\n",
    "# activate training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (config['train']):\n",
    "    print(\"Training model\", flush=True)\n",
    "    epochs = config['epochs']\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # setup loop with TQDM and dataloader\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for batch in loop:\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optim.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "            # print relevant info to progress bar\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "    model_scripted.save(f\"./trainedModels/model_E{config['epochs']}_{config['bert']}.pt\") # Save\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Multiprocessing\n",
    "\n",
    "'''    print(\"Model evaluation\\n\", flush = True)\n",
    "\n",
    "    for ud_file in glob.iglob(data_path + '**/*-test.txt', recursive=True):\n",
    "\n",
    "        ud_file = os.path.abspath(ud_file)\n",
    "        filename = os.path.basename(ud_file)\n",
    "        print(filename, flush = True)\n",
    "        # Load test data\n",
    "        with open(ud_file, 'r') as fp:\n",
    "            text.append(fp.read().split('\\n'))'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model evaluation\\n\", flush = True)\n",
    "text = \"\"\n",
    "for ud_file in glob.iglob(data_path + '**/*-test.txt', recursive=True):\n",
    "\n",
    "    ud_file = os.path.abspath(ud_file)\n",
    "    filename = os.path.basename(ud_file)\n",
    "    print(filename, flush = True)\n",
    "    # Load test data\n",
    "    with open(ud_file, 'r') as fp:\n",
    "        text.append(fp.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsTest = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "inputsTest = createMaskedInputs(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTest = SyntransDataset(inputsTest)\n",
    "loader = torch.utils.data.DataLoader(datasetTest, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['recall'] = 0\n",
    "results['precision'] = 0\n",
    "results['f1'] = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device).tolist()\n",
    "\n",
    "        softmax = nn.Softmax(dim = 2)\n",
    "        predictions = model(input_ids)\n",
    "        predictions = predictions['logits']\n",
    "        # predictions_sm = softmax(predictions)\n",
    "        # Get index of argmax\n",
    "        # y = np.argmax(predictions_sm, axis = -1)\n",
    "        # y = y.tolist()\n",
    "        y = torch.topk(predictions, k=1, dim = 2)[1].squeeze()\n",
    "        y = y.tolist()\n",
    "\n",
    "#        words_pred = []\n",
    "#        words_true = []\n",
    "#        for w in y:\n",
    "#            words_pred.append( tokenizer.convert_ids_to_tokens(w))\n",
    "        \n",
    "#        for w in labels[0]:\n",
    "#            words_true.append( tokenizer.convert_ids_to_tokens(w))\n",
    "        \n",
    "        #print(words_pred[0:50])\n",
    "        #print(words_true[0:50])\n",
    "            \n",
    "\n",
    "        recall_metric = evaluate.load('recall')\n",
    "        precision_metric = evaluate.load('precision')\n",
    "        f1_metric = evaluate.load('f1')\n",
    "\n",
    "        #print(np.shape(np.array(y)))\n",
    "        #Sprint(np.shape(np.array(labels)))\n",
    "\n",
    "        for idx, pred_batch in enumerate(y):\n",
    "\n",
    "            precision_metric.add_batch(references=labels[idx], predictions=pred_batch)\n",
    "            recall_metric.add_batch(references=labels[idx], predictions=pred_batch)\n",
    "            f1_metric.add_batch(references=labels[idx], predictions=pred_batch)\n",
    "\n",
    "    numberOfBatches = len(loop)\n",
    "    print(\"Results: untrained BERT-base\")\n",
    "    print(recall_metric.compute( average = 'micro'))\n",
    "    print(precision_metric.compute( average = 'micro'))\n",
    "    print(f1_metric.compute( average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir='out',\n",
    "#     per_device_train_batch_size=4,\n",
    "#     num_train_epochs=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=dataset\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv_syntrans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e86a731642ee256d624a4d29e8688bb9c6ad7b39856affb444c6cc9f38126795"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
