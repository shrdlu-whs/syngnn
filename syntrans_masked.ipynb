{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Export env vars to limit number of threads to use\n",
    "num_threads = \"26\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = num_threads \n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = num_threads\n",
    "os.environ[\"MKL_NUM_THREADS\"] = num_threads \n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = num_threads\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = num_threads\n",
    "\n",
    "# Only use CPU, hide GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#Import SummaryWriter for Tensorboard logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit no. of threads used by Pytorch\n",
    "torch.set_num_threads = int(num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID = os.getpid()\n",
    "PGID = os.getpgid(PID)\n",
    "print(f\"PID: {PID}, PGID: {PGID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['train'] = True\n",
    "config['epochs'] = 3\n",
    "config['batch_size'] = 128\n",
    "config['LR'] = 5e-5\n",
    "config['bert'] = 'bert-base-uncased'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/original/ud/\"\n",
    "writer = SummaryWriter()\n",
    "tokenizer = BertTokenizer.from_pretrained(config['bert'])\n",
    "model = BertForMaskedLM.from_pretrained(config['bert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMaskedInputs(inputs):\n",
    "    \"\"\"\n",
    "    creates masked input embeddings and labels from tokenized text\n",
    "\n",
    "    :param inputs: tokenized text\n",
    "    :return: masked input embeddings and new column labels \n",
    "    \"\"\" \n",
    "    # Clone input ids (tokens) to create labels\n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    # create mask array with 15% masked tokens\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "        (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "    # Select indices of each nonzero (= selected) value as token to be masked\n",
    "    selection = []\n",
    "\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    # Mask selected tokens: replace with [MASK] code 103 in tensor\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntransDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for ud_file in glob.iglob(data_path + '**/*-train.txt', recursive=True):\n",
    "\n",
    "  ud_file = os.path.abspath(ud_file)\n",
    "  filename = os.path.basename(ud_file)\n",
    "  print(filename, flush = True)\n",
    "  # Load train data\n",
    "  with open(ud_file, 'r') as fp:\n",
    "    text.extend(fp.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max sentence length\n",
    "max_length = 0\n",
    "for sentence in text:\n",
    "    length = len(sentence)\n",
    "    if(length > max_length):\n",
    "        max_length = length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap max sequence length at 512\n",
    "if(max_length > 512):\n",
    "    max_length = 512\n",
    "inputs = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n",
    "inputs = createMaskedInputs(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SyntransDataset(inputs)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)\n",
    "# activate training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=config['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (config['train']):\n",
    "    print(\"Training model\", flush=True)\n",
    "    epochs = config['epochs']\n",
    "    writer.add_scalar(\"LR\", config['LR'])\n",
    "    writer.add_scalar(\"Batchsize\", config['batch_size'])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        scalar_loss = 0\n",
    "        # setup loop with TQDM and dataloader\n",
    "        loop = tqdm(loader, leave=True, mininterval=40,maxinterval=120)\n",
    "        for batch in loop:\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optim.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "            # print relevant info to progress bar\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            scalar_loss = loss.item()\n",
    "            loop.set_postfix(loss=scalar_loss)\n",
    "        # Print info to Tensorboard\n",
    "        writer.add_scalar(\"Loss/train\", scalar_loss, epoch)\n",
    "        # Save model after each epoch\n",
    "        model.save_pretrained(save_directory=f\"./trained_models/E{epoch}_{config['bert']}_batches{config['batch_size']}_LR{config['LR']}_SL{max_length}/\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model evaluation\\n\", flush = True)\n",
    "text = []\n",
    "# Read test files\n",
    "for ud_file in glob.iglob(data_path + '**/*-test.txt', recursive=True):\n",
    "\n",
    "    ud_file = os.path.abspath(ud_file)\n",
    "    filename = os.path.basename(ud_file)\n",
    "    print(filename, flush = True)\n",
    "    # Load test data\n",
    "    with open(ud_file, 'r') as fp:\n",
    "        text.extend(fp.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsTest = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n",
    "inputsTest = createMaskedInputs(inputsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTest = SyntransDataset(inputsTest)\n",
    "loader = torch.utils.data.DataLoader(datasetTest, batch_size=config['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True, mininterval=20,maxinterval=120)\n",
    "    for batch in loop:\n",
    "        # Pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device).tolist()\n",
    "\n",
    "        softmax = nn.Softmax(dim = 2)\n",
    "        predictions = model(input_ids)\n",
    "        predictions = predictions['logits']\n",
    "        # predictions_sm = softmax(predictions)\n",
    "        # Get index of argmax\n",
    "        # y = np.argmax(predictions_sm, axis = -1)\n",
    "        # y = y.tolist()\n",
    "        y = torch.topk(predictions, k=1, dim = 2)[1].squeeze()\n",
    "        y = y.tolist()\n",
    "\n",
    "#        words_pred = []\n",
    "#        words_true = []\n",
    "#        for w in y:\n",
    "#            words_pred.append( tokenizer.convert_ids_to_tokens(w))\n",
    "        \n",
    "#        for w in labels[0]:\n",
    "#            words_true.append( tokenizer.convert_ids_to_tokens(w))\n",
    "        \n",
    "        #print(words_pred[0:50])\n",
    "        #print(words_true[0:50])\n",
    "            \n",
    "\n",
    "        recall_metric = evaluate.load('recall')\n",
    "        precision_metric = evaluate.load('precision')\n",
    "        f1_metric = evaluate.load('f1')\n",
    "        roc_auc_metric = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "\n",
    "        #print(np.shape(np.array(y)))\n",
    "        #Sprint(np.shape(np.array(labels)))\n",
    "\n",
    "        for idx, pred_batch in enumerate(y):\n",
    "\n",
    "            precision_metric.add_batch(references=labels[idx], predictions=pred_batch)\n",
    "            recall_metric.add_batch(references=labels[idx], predictions=pred_batch)\n",
    "            f1_metric.add_batch(references=labels[idx], predictions=pred_batch)\n",
    "            roc_auc_metric.add_batch(references=labels[idx], predictions=pred_batch)\n",
    "\n",
    "    numberOfBatches = len(loop)\n",
    "    print(f\"Results: {config['bert']}, Train={config['train']}\")\n",
    "    print(\"macro averaging\")\n",
    "    print(recall_metric.compute( average = 'macro'))\n",
    "    print(precision_metric.compute( average = 'macro'))\n",
    "    print(f1_metric.compute( average = 'macro'))\n",
    "    print(roc_auc_metric.compute( average = 'macro'))\n",
    "    print(\"weighted averaging\")\n",
    "    print(recall_metric.compute( average = 'weighted'))\n",
    "    print(precision_metric.compute( average = 'weighted'))\n",
    "    print(f1_metric.compute( average = 'weighted'))\n",
    "    print(roc_auc_metric.compute( average = 'weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv_syntrans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e86a731642ee256d624a4d29e8688bb9c6ad7b39856affb444c6cc9f38126795"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
