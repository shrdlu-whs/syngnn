{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: ner\n",
      "Model path: bert-base-cased\n",
      "Data path: ./data/ud/UD_English-GUM/\n",
      "Tokenizer: bert-base-cased\n",
      "Batch size: 2\n",
      "Epochs: 1\n",
      "Learning rate: 2e-05\n",
      "Sequence length: 96\n",
      "Training: True\n",
      "Num Threads: 4\n",
      "Num Sentences: 10\n",
      "Max Norm: 0.0\n",
      "Use GNN: True\n",
      "Num layers: 4\n",
      "Num attention heads: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import importlib\n",
    "import utilities as utils\n",
    "\n",
    "\n",
    "activeMode= \"develop\"\n",
    "\n",
    "# Reload utils library if changed\n",
    "importlib.reload(utils)\n",
    "\n",
    "configuration_csv = pd.read_csv(f\"./config/{activeMode}.csv\", dtype=str, sep=\";\")\n",
    "config = utils.configureParameters(configuration_csv)\n",
    "max_grad_norm_str = str(config.max_grad_norm).replace(\".\",\"-\")\n",
    "print(f\"Task: {config.task}\")\n",
    "print(f\"Model path: {config.saved_model_path}\")\n",
    "print(f\"Data path: {config.data_path}\")\n",
    "print(f\"Tokenizer: {config.tokenizer}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Epochs: {config.epochs}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Sequence length: {config.sequence_length}\")\n",
    "print(f\"Training: {config.train_model}\")\n",
    "print(f\"Num Threads: {config.num_threads}\")\n",
    "print(f\"Num Sentences: {config.num_sentences}\")\n",
    "print(f\"Max Norm: {config.max_grad_norm}\")\n",
    "print(f\"Use GNN: {config.use_gnn}\")\n",
    "if(config.use_gnn == True):\n",
    "    print(f\"Num layers: {config.num_layers}\")\n",
    "    print(f\"Num attention heads: {config.num_att_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Export env vars to limit number of threads to use\n",
    "num_threads = str(config.num_threads)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = num_threads \n",
    "os.environ[\"MKL_NUM_THREADS\"] = num_threads \n",
    "\n",
    "# Only use CPU, hide GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from '/home/shrdlu/cdaniel/syntrans/models.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models as models\n",
    "# Reload models if changed\n",
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertForTokenClassification, BertConfig, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#Import SummaryWriter for Tensorboard logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import (DataLoader, TensorDataset)\n",
    "# Load Pytorch Geometric\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.data as tg_data\n",
    "import torch_geometric.loader as tg_loader\n",
    "import torch_geometric.utils as tg_utils\n",
    "import torch_geometric.nn as tg_nn\n",
    "import torch.profiler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import evaluate\n",
    "# Evaluation metrics for NER task\n",
    "from seqeval.metrics import classification_report\n",
    "# Support for IOBES style NER labels\n",
    "from seqeval.scheme import IOBES\n",
    "import numpy as np\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "# Easy file reading\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 4617, PGID: 856\n"
     ]
    }
   ],
   "source": [
    "PID = os.getpid()\n",
    "PGID = os.getpgid(PID)\n",
    "print(f\"PID: {PID}, PGID: {PGID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 2\n",
      "\tat::get_num_interop_threads() : 2\n",
      "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "\tomp_get_max_threads() : 2\n",
      "Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 2\n",
      "Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)\n",
      "std::thread::hardware_concurrency() : 4\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : 4\n",
      "\tMKL_NUM_THREADS : 4\n",
      "ATen parallel backend: OpenMP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.__config__.parallel_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3712fc06f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit no. of threads used by Pytorch\n",
    "torch.set_num_threads = int(num_threads)\n",
    "torch.set_num_interop_threads = int(num_threads)\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 2\n",
      "\tat::get_num_interop_threads() : 2\n",
      "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "\tomp_get_max_threads() : 2\n",
      "Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 2\n",
      "Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)\n",
      "std::thread::hardware_concurrency() : 4\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : 4\n",
      "\tMKL_NUM_THREADS : 4\n",
      "ATen parallel backend: OpenMP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.__config__.parallel_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags_list = ['X','O','<unk>', 'B-CARDINAL', 'E-CARDINAL', 'S-PERSON', 'S-CARDINAL', 'S-PRODUCT', 'B-PRODUCT', 'I-PRODUCT', 'E-PRODUCT', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'E-WORK_OF_ART', 'B-PERSON', 'E-PERSON', 'S-GPE', 'B-DATE', 'I-DATE', 'E-DATE', 'S-ORDINAL', 'S-LANGUAGE', 'I-PERSON', 'S-EVENT', 'S-DATE', 'B-QUANTITY', 'E-QUANTITY', 'S-TIME', 'B-TIME', 'I-TIME', 'E-TIME', 'B-GPE', 'E-GPE', 'S-ORG', 'I-GPE', 'S-NORP', 'B-FAC', 'I-FAC', 'E-FAC', 'B-NORP', 'E-NORP', 'S-PERCENT', 'B-ORG', 'E-ORG', 'B-LANGUAGE', 'E-LANGUAGE', 'I-CARDINAL', 'I-ORG', 'S-WORK_OF_ART', 'I-QUANTITY', 'B-MONEY', 'I-MONEY', 'E-MONEY', 'B-LOC', 'E-LOC', 'I-LOC', 'B-PERCENT', 'I-PERCENT', 'E-PERCENT', 'S-LOC', 'S-FAC', 'B-EVENT', 'E-EVENT', 'I-EVENT', 'S-MONEY', 'B-LAW', 'I-LAW', 'E-LAW', 'I-NORP', 'I-LANGUAGE', 'S-LAW', 'S-QUANTITY', 'B-ORDINAL', 'I-ORDINAL', 'E-ORDINAL', '<START>', '<STOP>', \"[CLS]\", \"[SEP]\"]\n",
    "num_ner_labels = len(ner_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath for sentences\n",
    "filepath_train_data = config.data_path + '**/*-train.txt'\n",
    "filepath_validation_data = config.data_path + '**/*-dev.txt'\n",
    "filepath_test_data = config.data_path + '**/*-test.txt'\n",
    "\n",
    "# Filepath for syntax graphs\n",
    "filepath_train_syntrees = config.data_path + f'**/*-train-{config.tokenizer}.syntree'\n",
    "filepath_validation_syntrees = config.data_path + f'**/*-dev-{config.tokenizer}.syntree'\n",
    "filepath_test_syntrees = config.data_path + f'**/*-test-{config.tokenizer}.syntree'\n",
    "\n",
    "# Filepath for sentences with NER tags\n",
    "filepath_train_ner_labels = config.data_path + f'**/*-train-orig.ner'\n",
    "filepath_validation_ner_labels = config.data_path + f'**/*-dev-orig.ner'\n",
    "filepath_test_ner_labels = config.data_path + f'**/*-test-orig.ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (config.use_gnn == True):\n",
    "    tensorboard_dir = f\"./runs/{config.task}/synGNN/{config.tokenizer}_E{config.epochs}_batches{config.batch_size}_LR{config.learning_rate}_SL{config.sequence_length}_GN{max_grad_norm_str}_TL_{config.num_layers}AttH_{config.num_att_heads}\"\n",
    "    tensorboard_dir = utils.createNumberedDir(tensorboard_dir)\n",
    "\n",
    "    results_dir = f\"./logs/{config.task}/Results/synGNN/{config.task}_{config.tokenizer}_E{config.epochs}_batches{config.batch_size}_LR{config.learning_rate}_SL{config.sequence_length}_GN{max_grad_norm_str}_TL_{config.num_layers}AttH_{config.num_att_heads}\"\n",
    "    results_dir = utils.createNumberedDir(results_dir)\n",
    "    if (activeMode == \"prod\"):\n",
    "        trained_models_dir = f\"./trained_models/{config.task}/synGNN/{config.task}_{config.tokenizer}_E{config.epochs}_batches{config.batch_size}_LR{config.learning_rate}_SL{config.sequence_length}_GN{max_grad_norm_str}_TL_{config.num_layers}AttH_{config.num_att_heads}\"\n",
    "        trained_models_dir = utils.createNumberedDir(trained_models_dir)\n",
    "else:\n",
    "    tensorboard_dir = f\"./runs/{config.task}/bert/{config.tokenizer}_E{config.epochs}_batches{config.batch_size}_LR{config.learning_rate}_SL{config.sequence_length}_GN{max_grad_norm_str}\"\n",
    "    tensorboard_dir = utils.createNumberedDir(tensorboard_dir)\n",
    "\n",
    "    results_dir = f\"./logs/{config.task}/Results/bert/{config.task}_{config.tokenizer}_E{config.epochs}_batches{config.batch_size}_LR{config.learning_rate}_SL{config.sequence_length}_GN{max_grad_norm_str}\"\n",
    "    results_dir = utils.createNumberedDir(results_dir)\n",
    "    if (activeMode == \"prod\"):\n",
    "        trained_models_dir = f\"./trained_models/{config.task}/bert/{config.task}_{config.tokenizer}_E{config.epochs}_batches{config.batch_size}_LR{config.learning_rate}_SL{config.sequence_length}_GN{max_grad_norm_str}\"\n",
    "        trained_models_dir = utils.createNumberedDir(trained_models_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ud/UD_English-GUM/**/*-train-bert-base-cased.syntree\n"
     ]
    }
   ],
   "source": [
    "print(filepath_train_syntrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(config.tokenizer)\n",
    "if(config.task == 'mlm'):\n",
    "    model = BertForMaskedLM.from_pretrained(config.saved_model_path)\n",
    "if(config.task == 'ner'):\n",
    "    BERTconfig = BertConfig.from_pretrained(config.saved_model_path, num_labels=num_ner_labels, tokenizer = tokenizer)\n",
    "    if (config.use_gnn):\n",
    "        #model = models.SynBertForNer.from_pretrained(config.saved_model_path, num_node_features=1, num_labels = num_ner_labels, num_att_heads = 2, num_layers = 3, from_tf = False, config = BERTconfig, low_cpu_mem_usage=True)\n",
    "        model = models.SynBertForNer(bert_config = BERTconfig, num_node_features=768, num_labels = num_ner_labels, num_edge_attrs = 54, num_att_heads = config.num_att_heads, num_layers = config.num_layers)\n",
    "    else:\n",
    "        model = models.BertForNer.from_pretrained(config.saved_model_path, from_tf = False, config = BERTconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SynBertForNer(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (syngnn): SynGNN(\n",
       "    (layers): ModuleList(\n",
       "      (0): SynGNNLayer(\n",
       "        (graph_attn): GATv2Conv(768, 768, heads=2)\n",
       "        (linear1): Linear(768, 2048, bias=True)\n",
       "        (linear2): Linear(2048, 768, bias=True)\n",
       "        (linear_classifier): Linear(768, 79, bias=True)\n",
       "        (norm0): LayerNorm(768)\n",
       "        (norm1): LayerNorm(768)\n",
       "        (norm2): LayerNorm(768)\n",
       "        (norm3): LayerNorm(79)\n",
       "        (dropout0): Dropout(p=0.1, inplace=False)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): SynGNNLayer(\n",
       "        (graph_attn): GATv2Conv(768, 768, heads=2)\n",
       "        (linear1): Linear(768, 2048, bias=True)\n",
       "        (linear2): Linear(2048, 768, bias=True)\n",
       "        (linear_classifier): Linear(768, 79, bias=True)\n",
       "        (norm0): LayerNorm(768)\n",
       "        (norm1): LayerNorm(768)\n",
       "        (norm2): LayerNorm(768)\n",
       "        (norm3): LayerNorm(79)\n",
       "        (dropout0): Dropout(p=0.1, inplace=False)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): SynGNNLayer(\n",
       "        (graph_attn): GATv2Conv(768, 768, heads=2)\n",
       "        (linear1): Linear(768, 2048, bias=True)\n",
       "        (linear2): Linear(2048, 768, bias=True)\n",
       "        (linear_classifier): Linear(768, 79, bias=True)\n",
       "        (norm0): LayerNorm(768)\n",
       "        (norm1): LayerNorm(768)\n",
       "        (norm2): LayerNorm(768)\n",
       "        (norm3): LayerNorm(79)\n",
       "        (dropout0): Dropout(p=0.1, inplace=False)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): SynGNNLayer(\n",
       "        (graph_attn): GATv2Conv(768, 768, heads=2)\n",
       "        (linear1): Linear(768, 2048, bias=True)\n",
       "        (linear2): Linear(2048, 768, bias=True)\n",
       "        (linear_classifier): Linear(768, 79, bias=True)\n",
       "        (norm0): LayerNorm(768)\n",
       "        (norm1): LayerNorm(768)\n",
       "        (norm2): LayerNorm(768)\n",
       "        (norm3): LayerNorm(79)\n",
       "        (dropout0): Dropout(p=0.1, inplace=False)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device =  torch.device('cpu')\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, valid_ids=None, label_mask=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_inputs(inputs):\n",
    "    \"\"\"\n",
    "    creates masked input embeddings and labels from tokenized text\n",
    "\n",
    "    :param inputs: tokenized text\n",
    "    :return: masked input embeddings and new column labels \n",
    "    \"\"\" \n",
    "    # Clone input ids (tokens) to create labels\n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    # create mask array with 15% masked tokens\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "        (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "    # Select indices of each nonzero (= selected) value as token to be masked\n",
    "    selection = []\n",
    "\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    # Mask selected tokens: replace with [MASK] code 103 in tensor\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_input_features(sentence_labels_list, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a list of sentences into a list of input features for the transformer\n",
    "    \n",
    "        :return: list of inpt features objects\n",
    "    \"\"\"\n",
    "\n",
    "    # Map NER labels to indices\n",
    "    # start with 1: 0 reserved for invalid labels e.g. subtoken labels\n",
    "    label_map = {label : i for i, label in enumerate(label_list,0)}\n",
    "    #label_map['X'] = 0\n",
    "\n",
    "    features = []\n",
    "    for (sentence_idx,sentence_label_pair) in enumerate(sentence_labels_list):\n",
    "        # Tokenized text of sentence\n",
    "        tokens = []\n",
    "        # Token labels for sentence\n",
    "        labels = []\n",
    "        # Lists valid labels as 1 and labels to be ignored as 0 (e.g. for the labels for subword tokens which are not counting as separate labels for each token)\n",
    "        valid = []\n",
    "        # Mask for transformer indicating which tokens to ignore\n",
    "        label_mask = []\n",
    "        for word_label_pair in sentence_label_pair:\n",
    "            token = tokenizer.tokenize(word_label_pair[0])\n",
    "            tokens.extend(token)\n",
    "\n",
    "            label_word = word_label_pair[1]\n",
    "            for token_idx in range(len(token)):\n",
    "                # Append label for first token in word, mark as valid\n",
    "                if token_idx == 0:\n",
    "                    labels.append(label_word)\n",
    "                    valid.append(1)\n",
    "                    label_mask.append(1)\n",
    "                # Subword tokens: Mark as not valid\n",
    "                else:\n",
    "                    labels.append('X')\n",
    "                    valid.append(0)\n",
    "                    label_mask.append(1)\n",
    "        # Sentence exceeds max sequence length: cut to sequence length\n",
    "        if len(tokens) > max_seq_length-2:\n",
    "            if len(tokens)>90:\n",
    "                print(tokens)\n",
    "            tokens = tokens[0:(max_seq_length)]\n",
    "            labels = labels[0:(max_seq_length)]\n",
    "            valid = valid[0:(max_seq_length)]\n",
    "            label_mask = label_mask[0:(max_seq_length)]\n",
    "            if len(tokens)>90:\n",
    "                print(tokens)\n",
    "        # Tokens with BERT [CLS] and [SEP] tokens\n",
    "        ntokens = []\n",
    "        # Segment ids for BERT\n",
    "        segment_ids = []\n",
    "        # Label embedding ids for BERT\n",
    "        label_ids = []\n",
    "        # Start segment\n",
    "        ntokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        # Add CLS token label for Bert\n",
    "        label_ids.append(label_map[\"[CLS]\"])\n",
    "        # Mark as valid label\n",
    "        valid.insert(0,1)\n",
    "        label_mask.insert(0,1)\n",
    "\n",
    "        # add sentence tokens and label ids\n",
    "        for i, token in enumerate(tokens):\n",
    "            ntokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "            if len(labels) > i:\n",
    "                label_ids.append(label_map[labels[i]])\n",
    "        # End segment\n",
    "        ntokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        # Add SEP end token label for Bert\n",
    "        label_ids.append(label_map[\"[SEP]\"])\n",
    "        valid.append(1)\n",
    "        label_mask.append(1)\n",
    "\n",
    "\n",
    "        if(sentence_idx<2):\n",
    "            print(ntokens)\n",
    "\n",
    "        # Convert tokens to ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # Pad sentence to sequence length\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        # Pad labels to sequence length\n",
    "        while len(label_ids) < max_seq_length:\n",
    "            label_ids.append(0)\n",
    "            label_mask.append(0)\n",
    "            valid.append(1)\n",
    "        \n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "        assert len(valid) == max_seq_length\n",
    "        assert len(label_mask) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "        InputFeatures(\n",
    "                        input_ids=input_ids,\n",
    "                        input_mask=input_mask,\n",
    "                        segment_ids=segment_ids,\n",
    "                        label_ids=label_ids,\n",
    "                        valid_ids=valid,\n",
    "                        label_mask=label_mask\n",
    "                        ))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlmDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynGNNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, syntax_graphs,sentence_graph_idx_maps):\n",
    "\n",
    "        self.features = features\n",
    "        self.syntax_graphs = syntax_graphs\n",
    "        self.sentence_graph_idx_maps = sentence_graph_idx_maps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.syntax_graphs[idx]\n",
    "        if(self.features is not None):\n",
    "            features = self.features[idx]\n",
    "            return features, data, self.sentence_graph_idx_maps[idx]\n",
    "        else:\n",
    "            return data, self.sentence_graph_idx_maps[idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.syntax_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syngnn_data_collate_function(data):\n",
    "\n",
    "    # Get all Bert input feature objects\n",
    "    input_features = list(zip(*data))[0]\n",
    "    # Get all input ids\n",
    "    input_ids = torch.squeeze(torch.tensor(list(map(lambda x:x.input_ids,input_features)),dtype=torch.long))\n",
    "\n",
    "    # Get all input masks\n",
    "    input_masks = torch.squeeze(torch.tensor(list(map(lambda x:x.input_mask,input_features)),dtype=torch.long))\n",
    "\n",
    "    # Get all label ids\n",
    "    label_ids = torch.squeeze(torch.tensor(list(map(lambda x:x.label_ids,input_features)),dtype=torch.long))\n",
    "\n",
    "    # Get all valid ids\n",
    "    valid_ids = torch.squeeze(torch.tensor(list(map(lambda x:x.valid_ids,input_features)),dtype=torch.long))\n",
    "\n",
    "    # Get all label masks\n",
    "    label_masks = torch.squeeze(torch.tensor(list(map(lambda x:x.label_mask,input_features)),dtype=torch.long))\n",
    "\n",
    "    # Get all segment ids\n",
    "    segment_ids = torch.squeeze(torch.tensor(list(map(lambda x:x.segment_ids,input_features)),dtype=torch.long))\n",
    "\n",
    "    # Get all Pytorch Geom Data objects\n",
    "    pyg_data = list(zip(*data))[1]\n",
    "    #pyg_data = tg_data.Batch.from_data_list(pyg_data)\n",
    "\n",
    "    # Get all sentence_graph_idx_maps\n",
    "    sentence_graph_idx_maps = list(zip(*data))[2]\n",
    "\n",
    "\n",
    "    return input_ids, input_masks, label_ids, valid_ids, label_masks, segment_ids, pyg_data, sentence_graph_idx_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ud_tokenizer(tokenizer, tokenizer_name):\n",
    "    tokenizer_path = \"./tokenizers/\" + tokenizer_name \n",
    "    special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "    ]\n",
    "    # 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "    vocab_size = 30_522\n",
    "    # Load data\n",
    "    text = []\n",
    "    for ud_file in glob.iglob(config.data_path + '**/UD_English-Pronouns/en_*.txt', recursive=True):\n",
    "\n",
    "        ud_file = os.path.abspath(ud_file)\n",
    "        filename = os.path.basename(ud_file)\n",
    "        print(filename, flush = True)\n",
    "        tokenizer.train(files=ud_file, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "    # make the directory if not already there\n",
    "    if not os.path.isdir(tokenizer_path):\n",
    "        os.mkdir(tokenizer_path)\n",
    "    # save the tokenizer  \n",
    "    tokenizer.save_model(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences_from_files(filepath, seq_length=None):\n",
    "    \"\"\"\n",
    "    Load sentences from files.\n",
    "\n",
    "    :param filepath: path to files (supports glob regex)\n",
    "    :param seq_length: BERT sequence length. Must be provided when SynGNN is used instead of vanilla Bert. SynGNN only supports sentences up to seq_length, others are discarded\n",
    "    :return: list of sentences\n",
    "    \"\"\" \n",
    "    sentences = []\n",
    "    for ud_file in sorted(glob.iglob(filepath, recursive=True)):\n",
    "\n",
    "        ud_file = os.path.abspath(ud_file)\n",
    "        filename = os.path.basename(ud_file)\n",
    "        print(filename, flush = True)\n",
    "        with open(ud_file, 'r') as fp:\n",
    "            sentences_temp = fp.read().split('\\n')\n",
    "            if (seq_length != None):\n",
    "                for sentence in sentences_temp:\n",
    "                    if(len(sentence) <= seq_length):\n",
    "                        sentences.extend(sentence)\n",
    "            else:\n",
    "                sentences.extend(fp.read().split('\\n'))\n",
    "    return sentences\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_labels_from_files(filepath, num_sentences=0, excluded_idx_list=None):\n",
    "    \"\"\"\n",
    "    Load sentences from files.\n",
    "\n",
    "    :param filepath: path to files (supports glob regex)\n",
    "    :return: list of NER labels per sentence\n",
    "    \"\"\" \n",
    "    all_token_label_pairs = []\n",
    "    for ud_file in sorted(glob.iglob(filepath, recursive=True)):\n",
    "        sentences = []\n",
    "        ud_file = os.path.abspath(ud_file)\n",
    "        filename = os.path.basename(ud_file)\n",
    "        print(filename, flush = True)\n",
    "        with open(ud_file, 'r') as fp:\n",
    "            if (config.num_sentences == 0):\n",
    "                # Split labels file by sentences\n",
    "                sentences = fp.read().split('\\n')\n",
    "            else:\n",
    "                sentences = fp.read().split('\\n')[0:num_sentences]\n",
    "        # Split sentences by tokens\n",
    "        token_labels = [x.split(\"\\t\") for x in sentences]\n",
    "        # Remove empty line at end of sentence\n",
    "        [x.remove('') for x in token_labels]\n",
    "\n",
    "        if (excluded_idx_list != None):\n",
    "                       \n",
    "            token_labels = [x for idx, x in enumerate(token_labels) if idx not in excluded_idx_list]\n",
    "            \n",
    "        # Split token and NER tags\n",
    "        token_labels = [list(map(lambda x:x.split(\" \") ,tag_token)) for tag_token in token_labels]\n",
    "        tokens = [x[0] for idx, x in enumerate(token_labels)] \n",
    "\n",
    "        all_token_label_pairs.extend(token_labels)\n",
    "\n",
    "    return all_token_label_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_syntaxgraphs_from_files(filepath, seq_length=None, num_sentences=0):\n",
    "    r\"\"\"\n",
    "    Load binary syntax tree files (*.syntree).\n",
    "    Args:\n",
    "        filepath: path to files (supports glob regex)\n",
    "        return: list of pytorch geometric syntax graphs per file and list of sentence token to graph node mappings\n",
    "    \"\"\" \n",
    "    all_syntrees = []\n",
    "    all_sentence_to_graph_maps = []\n",
    "    for syntree_file in sorted(glob.iglob(filepath, recursive=True)):   \n",
    "        syntree_file = os.path.abspath(syntree_file)\n",
    "        filename = os.path.basename(syntree_file)\n",
    "        print(f\"Loading syntax graphs: from file {filename}\", flush=True)\n",
    "        with open(syntree_file, 'rb') as fp:\n",
    "            if (config.num_sentences == 0):\n",
    "                file_graphs_and_maps = pd.read_pickle(fp)\n",
    "            else:\n",
    "                file_graphs_and_maps = pd.read_pickle(fp)[0:num_sentences]\n",
    "\n",
    "            # Create new list where graph nodes <= seq_length\n",
    "            excluded_idx_list = []\n",
    "            file_graphs_and_maps_seq = []\n",
    "\n",
    "            for idx, graph_map_pair in enumerate(file_graphs_and_maps):\n",
    "                if graph_map_pair[0].x.shape[0]-1>=seq_length:\n",
    "                    excluded_idx_list.append(idx)\n",
    "                else:\n",
    "                    file_graphs_and_maps_seq.append(graph_map_pair)\n",
    "            #excluded_idx_list = [ idx for idx, graph_map_pair in enumerate(file_graphs_and_maps) if graph_map_pair[0].x.shape[0]-1>seq_length]\n",
    "            #file_graphs_and_maps = [ graph_map_pair for graph_map_pair in enumerate file_graphs_and_maps if graph_map_pair[0].x.shape[0]-1<=seq_length]\n",
    "\n",
    "            syntree_list = [graph_map_pair[0] for graph_map_pair in file_graphs_and_maps_seq]\n",
    "            sentence_to_graph_map_list = [graph_map_pair[1] for graph_map_pair in file_graphs_and_maps_seq]\n",
    "            all_syntrees.extend(syntree_list)\n",
    "            all_sentence_to_graph_maps.extend(sentence_to_graph_map_list)\n",
    "    print(\"Example syntax graphs:\")\n",
    "    print(all_syntrees[0:2])\n",
    "    print(all_sentence_to_graph_maps[0:2])\n",
    "    return all_syntrees, all_sentence_to_graph_maps, excluded_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxSequenceLength(sentences, cutoff_limit_percent=0.9999):\n",
    "    \"\"\"\n",
    "    Calculate maximum sequence length for given data.\n",
    "    param sentences: list of sentences\n",
    "    param cutoff_limit_percent: percentage of all samples to accommodate with the max sequence length.\n",
    "    returns: max sequence length which encompasses cutoff_limit_percent of all data samples\n",
    "    \"\"\"\n",
    "    # Get number of tokens per sentence        \n",
    "    max_sentence_tokens = 0\n",
    "    sentence_tokens = {}\n",
    "    print(f\"Amount of samples: {len(sentences)}\")\n",
    "    # Tokenize data\n",
    "    for sentence in sentences:\n",
    "\n",
    "        inputs = tokenizer(sentence, return_tensors='pt')\n",
    "        \n",
    "        token_count = inputs.input_ids.size(dim=1)\n",
    "        sentence_tokens[inputs.input_ids.size(dim=1)] = sentence_tokens.get(token_count,0) + 1\n",
    "        if(token_count > max_sentence_tokens): \n",
    "            max_sentence_tokens = token_count\n",
    "            \n",
    "    no_tokens = 0\n",
    "    # Calulate number of samples which should have a sequence length smaller than max_sequence_length\n",
    "    cutoff = cutoff_limit_percent * len(sentences)\n",
    "    max_sequence_length = 0\n",
    "    print(max_sentence_tokens)\n",
    "    for i in sorted(sentence_tokens):\n",
    "        # print((i, sentence_tokens[i]), end=\" \")\n",
    "        if(no_tokens <= cutoff):\n",
    "            no_tokens = no_tokens + sentence_tokens[i]\n",
    "            max_sequence_length = i\n",
    "\n",
    "    print(f\"Max sequence length: {max_sequence_length} with {cutoff_limit_percent}% of samples smaller\")\n",
    "    return max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataloader(filepath, filepath_syntrees = None, shuffle_data=False):\n",
    "\n",
    "        # Load syntax graphs and create dataset\n",
    "        if(config.use_gnn and filepath_syntrees != None):\n",
    "            syntax_graphs, sentence_to_graph_idx_maps, excluded_idx_list = load_syntaxgraphs_from_files(filepath_syntrees, config.sequence_length, config.num_sentences)\n",
    "        # Load NER labels\n",
    "        if(config.task == 'ner'):\n",
    "            print(f\"Loading NER labels from {filepath}\")\n",
    "            # Load NER labels\n",
    "            sentence_labels_list = load_ner_labels_from_files(filepath, config.num_sentences, excluded_idx_list)\n",
    "            num_sentences = len(sentence_labels_list)\n",
    "            num_graphs = len(syntax_graphs)\n",
    "            \n",
    "            if num_sentences % config.batch_size == 1:\n",
    "                 syntax_graphs.pop(-1)\n",
    "                 sentence_labels_list.pop(-1)\n",
    "                 sentence_to_graph_idx_maps.pop(-1)\n",
    "                 num_sentences = len(sentence_labels_list)\n",
    "                 num_graphs = len(syntax_graphs)\n",
    "            \n",
    "            num_batches = math.ceil(num_sentences / config.batch_size)\n",
    "\n",
    "            print(f\"{num_sentences} sentences, {num_graphs} graphs, {num_batches} batches of size {config.batch_size}\")\n",
    "            print(f\"Example of NER labels: {sentence_labels_list[0:2]}\")\n",
    "\n",
    "            features = create_ner_input_features(sentence_labels_list, ner_tags_list, config.sequence_length, tokenizer)\n",
    "            all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "            all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "            all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "            all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "            all_valid_ids = torch.tensor([f.valid_ids for f in features], dtype=torch.long)\n",
    "            all_lmask_ids = torch.tensor([f.label_mask for f in features], dtype=torch.long)\n",
    "            \n",
    "            data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)\n",
    "            #print(data.__sizeof__())\n",
    "\n",
    "            # Print control example of InputFeatures\n",
    "            print(\"Control example of InputFeatures\")\n",
    "            print(f\"Input Ids: {str(features[1].input_ids)}\")\n",
    "            print(f\"Input Mask: {str(features[1].input_mask)}\")\n",
    "            print(f\"Label Ids: {str(features[1].label_ids)}\")\n",
    "            print(f\"Valid Ids: {str(features[1].valid_ids)}\")\n",
    "            print(f\"Label Mask: {str(features[1].label_mask)}\")\n",
    "            print(f\"Segment Ids: {str(features[1].segment_ids)}\")\n",
    "\n",
    "\n",
    "        if(config.task == 'mlm'):\n",
    "            # Load data\n",
    "            sentences = load_sentences_from_files(filepath)\n",
    "            # Tokenize data\n",
    "            inputs = tokenizer(sentences, return_tensors='pt', max_length=config.sequence_length, truncation=True, padding='max_length')\n",
    "            inputs = create_masked_inputs(inputs)\n",
    "\n",
    "            # Create dataset from tokenized data\n",
    "            data = MlmDataset(inputs)\n",
    "        \n",
    "        # Create dataset\n",
    "        if(config.use_gnn and filepath_syntrees != None):\n",
    "            syngnn_dataset = SynGNNDataset(features, syntax_graphs, sentence_to_graph_idx_maps)\n",
    "            \n",
    "\n",
    "            print(f\"Syngnn dataset size: {syngnn_dataset.__len__()}\")\n",
    "\n",
    "            loader = DataLoader(syngnn_dataset, batch_size=config.batch_size, shuffle=shuffle_data, collate_fn=syngnn_data_collate_function)\n",
    "        else:\n",
    "            loader = DataLoader(data, batch_size=config.batch_size, shuffle=shuffle_data)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainData():\n",
    "    print(\"Loading Training Data\")\n",
    "    if(config.task == 'ner'):\n",
    "        if(config.use_gnn):\n",
    "            return createDataloader(filepath_train_ner_labels, filepath_train_syntrees, shuffle_data=True)\n",
    "        else:\n",
    "            return createDataloader(filepath_train_ner_labels, shuffle_data=True)\n",
    "    if(config.task == 'mlm'):\n",
    "        if(config.use_gnn):\n",
    "            return createDataloader(filepath_train_data, filepath_train_syntrees, shuffle_data=True)\n",
    "        else:\n",
    "            return createDataloader(filepath_train_data, shuffle_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadValidationData():\n",
    "    print(\"Loading Validation Data\")\n",
    "    if(config.task == 'ner'):\n",
    "        if(config.use_gnn):\n",
    "            return createDataloader(filepath_validation_ner_labels, filepath_validation_syntrees, shuffle_data=True)\n",
    "        else:\n",
    "            return createDataloader(filepath_validation_ner_labels, shuffle_data=True)\n",
    "    if(config.task == 'mlm'):\n",
    "        return createDataloader(filepath_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestData():\n",
    "    print(\"Test Data\")\n",
    "    if(config.task == 'ner'):\n",
    "        if(config.use_gnn):\n",
    "            return createDataloader(filepath_test_ner_labels, filepath_test_syntrees, shuffle_data=False)\n",
    "        else:\n",
    "            return createDataloader(filepath_test_ner_labels, shuffle_data=False)\n",
    "    if(config.task == 'mlm'):\n",
    "        return createDataloader(filepath_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(epoch, trainLoader, writer):\n",
    "    # activate training mode\n",
    "    model.train()\n",
    "\n",
    "    from torch.optim import AdamW\n",
    "    # initialize optimizer\n",
    "    optim = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(trainLoader, leave=True, mininterval=40,maxinterval=120)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        if (config.task == 'ner'):\n",
    "            batch = tuple(t for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids, valid_ids,l_mask = batch\n",
    "            batch_loss, logits = model(input_ids, segment_ids, input_mask, label_ids,valid_ids,l_mask)\n",
    "            batch_loss = loss.item()\n",
    "        if (config.task == 'mlm'):\n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Train Epoch {epoch}')\n",
    "        loop.set_postfix(loss=batch_loss)\n",
    "        epoch_loss = epoch_loss + batch_loss\n",
    "    # Calculate epoch loss\n",
    "    epoch_loss = epoch_loss / len(trainLoader)\n",
    "    # Print info to Tensorboard\n",
    "    writer.add_scalar(\"Loss\", epoch_loss, epoch)\n",
    "    return epoch_loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateModel(epoch, validationLoader, writer):\n",
    "    # activate eval mode\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    # setup loop with TQDM and dataloader\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(validationLoader, leave=True, mininterval=40,maxinterval=120)\n",
    "        for batch in loop:\n",
    "            # pull all tensor batches required for training\n",
    "            if (config.task == 'ner'):\n",
    "                batch = tuple(t for t in batch)\n",
    "                input_ids, input_mask, segment_ids, label_ids, valid_ids,l_mask = batch\n",
    "                batch_loss, logits = model(input_ids, segment_ids, input_mask, label_ids,valid_ids,l_mask)\n",
    "                batch_loss = batch_loss.item()\n",
    "            if (config.task == 'mlm'):\n",
    "                # pull all tensor batches required for training\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                # process\n",
    "                batch_loss, logits = model(input_ids, attention_mask=attention_mask,\n",
    "                                labels=labels)\n",
    "                batch_loss = batch_loss.item()\n",
    "\n",
    "            # print relevant info to progress bar\n",
    "            loop.set_description(f'Validation Epoch {epoch}')\n",
    "\n",
    "            loop.set_postfix(loss=batch_loss)\n",
    "            epoch_loss = epoch_loss + batch_loss\n",
    "\n",
    "            # Calculate epoch loss\n",
    "            epoch_loss = epoch_loss / len(validationLoader)\n",
    "            #print(epoch_loss)\n",
    "            # Print info to Tensorboard\n",
    "            writer.add_scalar(\"Loss\", epoch_loss, epoch)\n",
    "            return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stderr\n",
    "prof = torch.profiler.profile(\n",
    "        activities=[ProfilerActivity.CPU],\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=1, repeat=1),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./profiler/noembeddings'),\n",
    "        record_shapes=False,\n",
    "        with_stack=True)\n",
    "def runModelGNN(data_loader, mode=None, writer = None, results_dir = None,  epoch = None):\n",
    "\n",
    "    if(mode == 'Train'):\n",
    "        model.train()\n",
    "        from torch.optim import AdamW\n",
    "        # initialize optimizer\n",
    "        optim = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    elif(mode == 'Test' or mode == 'Validation'):\n",
    "        model.eval()\n",
    "    else:\n",
    "        stderr(\"Mode must be Train, Validation or Test\")\n",
    "        exit()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    label_map = {i : label for i, label in enumerate(ner_tags_list,0)}\n",
    "\n",
    "    references_all = []\n",
    "    predictions_all = []\n",
    "    references_roc_all = []\n",
    "    predictions_roc_all = []\n",
    "\n",
    "    if(config.task == 'mlm'):\n",
    "        # Setup loop with TQDM and dataloader\n",
    "        loop = tqdm(data_loader, leave=True, mininterval=20,maxinterval=120)\n",
    "        for batch in loop:\n",
    "    \n",
    "            # Pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device).tolist()\n",
    "\n",
    "            softmax = nn.Softmax(dim = -1)\n",
    "            if (mode == 'Test' or mode == 'Validation'):\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(input_ids)\n",
    "            if(mode == 'Train'):\n",
    "                # initialize calculated gradients (from prev step)\n",
    "                optim.zero_grad()\n",
    "                predictions = model(input_ids)\n",
    "            predictions = predictions['logits']\n",
    "            predictions_sm = softmax(predictions)\n",
    "\n",
    "            # Change type to double to prevent floating point rounding errors\n",
    "            predictions = predictions.type(torch.float64)\n",
    "            predictions_sm = softmax(predictions)\n",
    "\n",
    "            # Get index of argmax\n",
    "            y = torch.topk(predictions, k=1, dim = 2)[1].squeeze()\n",
    "            y = y.tolist()\n",
    "                \n",
    "            recall_metric = evaluate.load('recall')\n",
    "            precision_metric = evaluate.load('precision')\n",
    "            f1_metric = evaluate.load('f1')\n",
    "            roc_auc_metric = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "\n",
    "            # Go through all samples in batch and add to computation batch\n",
    "            for idx, pred_batch in enumerate(y):\n",
    "                references_all.extend(labels[idx])\n",
    "                predictions_all.extend(pred_batch)\n",
    "            \n",
    "            # Calculate ROC (TODO)\n",
    "            #for batch_idx, pred_batch in enumerate(predictions_sm):\n",
    "            #    predictions_roc_all.extend(pred_batch.tolist())\n",
    "            #    references_roc_all.extend(labels[batch_idx])\n",
    "            #    #roc_auc_metric.add_batch(references=labels[batch_idx], prediction_scores = pred_batch.tolist())\n",
    "            #    break\n",
    "            #break\n",
    "\n",
    "        numberOfBatches = len(loop)\n",
    "        # List all possible labels\n",
    "        labels = np.arange(tokenizer.vocab_size)\n",
    "        with open(results_dir +\"results.txt\", \"w\") as output:\n",
    "            print(f\"Results: {config.tokenizer}, Train={config.train_model} {config.tokenizer}_E{config.epochs}_batches{config.batch_size}_LR{config.learning_rate}_SL{config.sequence_length}\", file = output)\n",
    "            output.write(\"macro averaging\\n\")\n",
    "            output.write(str(recall_metric.compute(references = references_all, predictions = predictions_all, average = 'macro')))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(precision_metric.compute(references = references_all, predictions = predictions_all, average = 'macro', zero_division = 0)))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(f1_metric.compute( references = references_all, predictions = predictions_all, average = 'macro')))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(roc_auc_metric.compute( references = references_roc_all, prediction_scores = predictions_roc_all, average = 'macro', multi_class = 'ovo', labels = labels, max_fpr = 1.0)))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(\"weighted averaging\\n\")\n",
    "            output.write(str(recall_metric.compute( references = references_all, predictions = predictions_all, average = 'weighted')))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(precision_metric.compute( references = references_all, predictions = predictions_all, average = 'weighted', zero_division = 0)))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(f1_metric.compute( references = references_all, predictions = predictions_all, average = 'weighted')))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(roc_auc_metric.compute( references = references_roc_all, prediction_scores = predictions_roc_all, average = 'weighted', multi_class = 'ovo', labels = labels, max_fpr = 1.0)))\n",
    "            output.close()\n",
    "    \n",
    "    if (config.task == 'ner'):\n",
    "\n",
    "        sep_token_id = int(ner_tags_list.index(\"[SEP]\"))\n",
    "        cls_token_id = int(ner_tags_list.index(\"[CLS]\"))\n",
    "        unk_token_id = int(ner_tags_list.index(\"<unk>\"))\n",
    "        O_token_id = int(ner_tags_list.index(\"O\"))\n",
    "\n",
    "        special_token_predictions = 0\n",
    "        O_token_predictions = 0\n",
    "        ner_token_predictions = 0\n",
    "        epoch_loss = 0\n",
    "        \n",
    "\n",
    "        \n",
    "        # setup loop with TQDM and dataloader\n",
    "        loop = tqdm(data_loader, leave=True, mininterval=20,maxinterval=120)\n",
    "        # Loop over all batches\n",
    "        prof.start()\n",
    "        for batch in loop:\n",
    "            batch = tuple(t for t in batch)\n",
    "            if (config.use_gnn):\n",
    "                input_ids, input_masks, label_ids, valid_ids, label_mask, segment_ids, pyg_data, sentence_graph_idx_maps = batch\n",
    "                input_ids = input_ids.to(device)\n",
    "                input_masks = input_masks.to(device)\n",
    "                segment_ids = segment_ids.to(device)\n",
    "                label_ids = label_ids.to(device)\n",
    "                valid_ids = valid_ids.to(device)\n",
    "                label_mask = label_mask.to(device)\n",
    "            else:\n",
    "                input_ids, input_masks, segment_ids, label_ids, valid_ids,label_mask = batch\n",
    "                input_ids = input_ids.to(device)\n",
    "                input_masks = input_masks.to(device)\n",
    "                segment_ids = segment_ids.to(device)\n",
    "                label_ids = label_ids.to(device)\n",
    "                valid_ids = valid_ids.to(device)\n",
    "                label_mask = label_mask.to(device)\n",
    "\n",
    "            if(mode == 'Train'):\n",
    "                # initialize gradients for batch to zero\n",
    "                optim.zero_grad()\n",
    "                if (config.use_gnn):\n",
    "                    loss, logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_masks, attention_mask_label=label_mask,label_ids=label_ids,valid_ids=valid_ids,syntax_graphs=pyg_data, sentence_graph_idx_maps=sentence_graph_idx_maps)\n",
    "                    prof.step()\n",
    "                else:\n",
    "                    loss, logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_masks, attention_mask_label=label_mask,label_ids=label_ids,valid_ids=valid_ids)\n",
    "                # calculate loss for every parameter that needs grad update\n",
    "                loss.backward()\n",
    "                if (config.max_grad_norm):\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                # update parameters\n",
    "                optim.step()\n",
    "            elif(mode == 'Validation'):\n",
    "                with torch.no_grad():\n",
    "                    if (config.use_gnn):\n",
    "                        loss, logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_masks, attention_mask_label=label_mask,label_ids=label_ids,valid_ids=valid_ids,syntax_graphs=pyg_data, sentence_graph_idx_maps=sentence_graph_idx_maps)\n",
    "                    else:\n",
    "                        loss, logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_masks, attention_mask_label=label_mask,label_ids=label_ids,valid_ids=valid_ids)\n",
    "            elif(mode == 'Test'):\n",
    "                if (config.use_gnn):\n",
    "                    logits = model(input_ids=input_ids, token_type_ids=segment_ids,attention_mask=input_masks,valid_ids=valid_ids,attention_mask_label=label_mask, syntax_graphs=pyg_data, sentence_graph_idx_maps=sentence_graph_idx_maps)\n",
    "                else:\n",
    "                    logits = model(input_ids, segment_ids, input_masks,valid_ids=valid_ids,attention_mask_label=label_mask)\n",
    "\n",
    "            if(mode == 'Validation' or mode == 'Train'):\n",
    "                # print relevant info to progress bar\n",
    "                loop.set_description(f'{mode} Epoch {epoch}')\n",
    "                batch_loss = loss.item()\n",
    "                loop.set_postfix(loss=batch_loss)\n",
    "                epoch_loss = epoch_loss + batch_loss\n",
    "            \n",
    "            #print(f\"Logits{logits}\")\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "\n",
    "            # Get highest NER label prediction for all sentences\n",
    "            #print(f\"Softmax:{softmax(logits)}\")\n",
    "            logits = torch.argmax(softmax(logits),dim=1)\n",
    "            #print(f\"Argmax: {logits}\")\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            input_masks = input_masks.to('cpu').numpy()\n",
    "\n",
    "            # Go through true labels\n",
    "            for label_list_idx, true_sentence_labels in enumerate(label_ids):\n",
    "                y_true_temp = []\n",
    "                y_pred_temp = []\n",
    "\n",
    "                for label_idx, label_id in enumerate(true_sentence_labels):\n",
    "\n",
    "                    # Skip 0 label\n",
    "                    if label_id == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Skip [CLS] label at sequence beginning\n",
    "                    if label_id == cls_token_id:\n",
    "                        continue\n",
    "\n",
    "                    # Detect [SEP] label at sentence end and ignore [SEP] and all sequence padding\n",
    "                    elif label_id == sep_token_id:\n",
    "                        y_true.append(y_true_temp)\n",
    "                        y_pred.append(y_pred_temp)\n",
    "                        break\n",
    "                    else:\n",
    "                        # Predicted NER label is special token: count preds\n",
    "                        #if (logits[label_list_idx][label_idx] == 0):\n",
    "                        if (logits[label_list_idx] == 0):\n",
    "                            special_token_predictions = special_token_predictions +1\n",
    "                        # Predicted NER label is O: count preds\n",
    "                        #elif (logits[label_list_idx][label_idx] == O_token_id):\n",
    "                        elif (logits[label_list_idx] == O_token_id):\n",
    "                            O_token_predictions = O_token_predictions +1\n",
    "                        else:\n",
    "                            ner_token_predictions = ner_token_predictions +1\n",
    "\n",
    "                        # Append label and prediction to list\n",
    "                        y_true_temp.append(label_map[label_id])\n",
    "                        #y_pred_temp.append(label_map[logits[label_list_idx][label_idx]])\n",
    "                        y_pred_temp.append(label_map[logits[label_list_idx]])\n",
    "\n",
    "        if (mode == 'Train' or mode == 'Validation'):\n",
    "            report = classification_report(y_true, y_pred, digits=4, output_dict=True, zero_division = 0)\n",
    "            # Calculate epoch loss\n",
    "            epoch_loss = epoch_loss / len(data_loader)\n",
    "\n",
    "            # Print info to Tensorboard\n",
    "            writer.add_scalar(\"Loss\", epoch_loss, epoch)\n",
    "            macro_precision = report['macro avg']['precision']\n",
    "            writer.add_scalar(\"macro_avg/precision\", macro_precision, epoch)\n",
    "            macro_recall = report['macro avg']['recall']\n",
    "            writer.add_scalar(\"macro_avg/recall\", macro_recall, epoch)\n",
    "            macro_f1 = report['macro avg']['f1-score']\n",
    "            writer.add_scalar(\"macro_avg/f1\", macro_f1, epoch)\n",
    "\n",
    "            weighted_precision = report['weighted avg']['precision']\n",
    "            writer.add_scalar(\"weighted_avg/precision\", weighted_precision, epoch)\n",
    "            weighted_recall = report['weighted avg']['recall']\n",
    "            writer.add_scalar(\"weighted_avg/recall\", weighted_recall, epoch)\n",
    "            weighted_f1 = report['weighted avg']['f1-score']\n",
    "            writer.add_scalar(\"weighted_avg/f1\", weighted_f1, epoch)\n",
    "            print(f\"O Token Predictions: {O_token_predictions}, NER token predictions: {ner_token_predictions}\")\n",
    "            print(f\"loss: {epoch_loss} w prec: {weighted_precision} w recall: {weighted_recall} w f1: {weighted_f1}\")\n",
    "            return epoch_loss, macro_precision, macro_recall, macro_f1, weighted_precision, weighted_recall, weighted_f1\n",
    "\n",
    "        else:\n",
    "            print(y_true)\n",
    "            print(y_pred)\n",
    "            report = classification_report(y_true, y_pred, digits=4, output_dict=False)\n",
    "            with open(results_dir +\"results.txt\", \"w\") as output:\n",
    "                print(\"***** Test results *****\")\n",
    "                print(f\"Task: {config.task}\")\n",
    "                print(f\"Model path: {config.saved_model_path}\")\n",
    "                print(f\"Data path: {config.data_path}\")\n",
    "                print(f\"Tokenizer: {config.tokenizer}\")\n",
    "                print(f\"Batch size: {config.batch_size}\")\n",
    "                print(f\"Epochs: {config.epochs}\")\n",
    "                print(f\"Learning rate: {config.learning_rate}\")\n",
    "                print(f\"Sequence length: {config.sequence_length}\")\n",
    "                print(f\"Training: {config.train_model}\")\n",
    "                print(f\"Num Threads: {config.num_threads}\")\n",
    "                print(f\"Num Sentences: {config.num_sentences}\")\n",
    "                print(f\"Max Grad Norm: {config.max_grad_norm}\")\n",
    "                print(f\"{report}\\n Special token predictions: {special_token_predictions}\")\n",
    "                output.write(report)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stderr\n",
    "def evaluateModel(data_loader, mode=None, writer = None, results_dir = None,  epoch = None):\n",
    "    if(mode == 'Train'):\n",
    "        model.train()\n",
    "        from torch.optim import AdamW\n",
    "        # initialize optimizer\n",
    "        optim = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    elif(mode == 'Test' or mode == 'Validation'):\n",
    "        model.eval()\n",
    "    else:\n",
    "        stderr(\"Mode must be Train, Validation or Test\")\n",
    "        exit()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    label_map = {i : label for i, label in enumerate(ner_tags_list,0)}\n",
    "\n",
    "    references_all = []\n",
    "    predictions_all = []\n",
    "    references_roc_all = []\n",
    "    predictions_roc_all = []\n",
    "\n",
    "    if(config.task == 'mlm'):\n",
    "        # Setup loop with TQDM and dataloader\n",
    "        loop = tqdm(data_loader, leave=True, mininterval=20,maxinterval=120)\n",
    "        for batch in loop:\n",
    "    \n",
    "            # Pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device).tolist()\n",
    "\n",
    "            softmax = nn.Softmax(dim = -1)\n",
    "            if (mode == 'Test' or mode == 'Validation'):\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(input_ids)\n",
    "            if(mode == 'Train'):\n",
    "                # initialize calculated gradients (from prev step)\n",
    "                optim.zero_grad()\n",
    "                predictions = model(input_ids)\n",
    "            predictions = predictions['logits']\n",
    "            predictions_sm = softmax(predictions)\n",
    "\n",
    "            # Change type to double to prevent floating point rounding errors\n",
    "            predictions = predictions.type(torch.float64)\n",
    "            predictions_sm = softmax(predictions)\n",
    "\n",
    "            # Get index of argmax\n",
    "            #y = np.argmax(predictions_sm, axis = -1)\n",
    "            # y = y.tolist()\n",
    "            y = torch.topk(predictions, k=1, dim = 2)[1].squeeze()\n",
    "            y = y.tolist()\n",
    "                \n",
    "\n",
    "            recall_metric = evaluate.load('recall')\n",
    "            precision_metric = evaluate.load('precision')\n",
    "            f1_metric = evaluate.load('f1')\n",
    "            roc_auc_metric = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "\n",
    "\n",
    "            # Go through all samples in batch and add to computation batch\n",
    "            for idx, pred_batch in enumerate(y):\n",
    "                references_all.extend(labels[idx])\n",
    "                predictions_all.extend(pred_batch)\n",
    "                #precision_metric.add_batch(references=labels[idx], predictions=pred_batch)\n",
    "                #recall_metric.add_batch(references=labels[idx], predictions=pred_batch)\n",
    "                #f1_metric.add_batch(references=labels[idx], predictions=pred_batch)\n",
    "            \n",
    "            # Calculate ROC\n",
    "            for batch_idx, pred_batch in enumerate(predictions_sm):\n",
    "                predictions_roc_all.extend(pred_batch.tolist())\n",
    "                references_roc_all.extend(labels[batch_idx])\n",
    "                #roc_auc_metric.add_batch(references=labels[batch_idx], prediction_scores = pred_batch.tolist())\n",
    "                break\n",
    "            break\n",
    "\n",
    "        numberOfBatches = len(loop)\n",
    "        # List all possible labels\n",
    "        labels = np.arange(tokenizer.vocab_size)\n",
    "        with open(results_dir +\"results.txt\", \"w\") as output:\n",
    "            print(f\"Results: {config.tokenizer}, Train={config.train_model} {config.tokenizer}_E{config.epochs}_batches{config.batch_size}_LR{config.learning_rate}_SL{config.sequence_length}\", file = output)\n",
    "            output.write(\"macro averaging\\n\")\n",
    "            output.write(str(recall_metric.compute(references = references_all, predictions = predictions_all, average = 'macro')))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(precision_metric.compute(references = references_all, predictions = predictions_all, average = 'macro', zero_division = 0)))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(f1_metric.compute( references = references_all, predictions = predictions_all, average = 'macro')))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(roc_auc_metric.compute( references = references_roc_all, prediction_scores = predictions_roc_all, average = 'macro', multi_class = 'ovo', labels = labels, max_fpr = 1.0)))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(\"weighted averaging\\n\")\n",
    "            output.write(str(recall_metric.compute( references = references_all, predictions = predictions_all, average = 'weighted')))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(precision_metric.compute( references = references_all, predictions = predictions_all, average = 'weighted', zero_division = 0)))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(f1_metric.compute( references = references_all, predictions = predictions_all, average = 'weighted')))\n",
    "            output.write(\"\\n\")\n",
    "            output.write(str(roc_auc_metric.compute( references = references_roc_all, prediction_scores = predictions_roc_all, average = 'weighted', multi_class = 'ovo', labels = labels, max_fpr = 1.0)))\n",
    "            output.close()\n",
    "    \n",
    "    if (config.task == 'ner'):\n",
    "\n",
    "        sep_token_id = int(ner_tags_list.index(\"[SEP]\"))\n",
    "        cls_token_id = int(ner_tags_list.index(\"[CLS]\"))\n",
    "        unk_token_id = int(ner_tags_list.index(\"<unk>\"))\n",
    "        O_token_id = int(ner_tags_list.index(\"O\"))\n",
    "\n",
    "        special_token_predictions = 0\n",
    "        O_token_predictions = 0\n",
    "        ner_token_predictions = 0\n",
    "        epoch_loss = 0\n",
    "        # setup loop with TQDM and dataloader\n",
    "        loop = tqdm(data_loader, leave=True, mininterval=20,maxinterval=120)\n",
    "        # Loop over all batches\n",
    "        for batch in loop:\n",
    "            batch = tuple(t for t in batch)\n",
    "            if (config.use_gnn):\n",
    "                input_ids, input_masks, segment_ids, label_ids, valid_ids,l_mask, syntax_graphs, sentence_graph_idx_maps = batch\n",
    "                \n",
    "            else:\n",
    "                input_ids, input_masks, segment_ids, label_ids, valid_ids,l_mask = batch\n",
    "            if(mode == 'Train'):\n",
    "                # initialize calculated gradients (from prev step)\n",
    "                optim.zero_grad()\n",
    "                loss, logits = model(input_ids, segment_ids, input_masks, label_ids,valid_ids,l_mask)\n",
    "                # calculate loss for every parameter that needs grad update\n",
    "                loss.backward()\n",
    "                if (config.max_grad_norm):\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                # update parameters\n",
    "                optim.step()\n",
    "            elif(mode == 'Validation'):\n",
    "                with torch.no_grad():\n",
    "                    loss, logits = model(input_ids, segment_ids, input_masks, label_ids,valid_ids,l_mask)\n",
    "            elif(mode == 'Test'):\n",
    "                if (config.use_gnn):\n",
    "                     logits = model(input_ids=input_ids, token_type_ids=segment_ids,attention_mask=input_masks,valid_ids=valid_ids,attention_mask_label=l_mask, syntax_graphs=syntax_graphs, sentence_graph_idx_maps=sentence_graph_idx_maps)\n",
    "                else:\n",
    "                    logits = model(input_ids, segment_ids, input_masks,valid_ids=valid_ids,attention_mask_label=l_mask)\n",
    "\n",
    "            if(mode == 'Validation' or mode == 'Train'):\n",
    "                # print relevant info to progress bar\n",
    "                loop.set_description(f'{mode} Epoch {epoch}')\n",
    "                batch_loss = loss.item()\n",
    "                loop.set_postfix(loss=batch_loss)\n",
    "                epoch_loss = epoch_loss + batch_loss\n",
    "            \n",
    "            #print(f\"Logits{logits}\")\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "\n",
    "            # Get highest NER label prediction for all sentences\n",
    "            #print(f\"Softmax:{softmax(logits)}\")\n",
    "            logits = torch.argmax(softmax(logits),dim=1)\n",
    "            #print(f\"Argmax: {logits}\")\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            input_masks = input_masks.to('cpu').numpy()\n",
    "\n",
    "            # Go through true labels\n",
    "            for label_list_idx, true_sentence_labels in enumerate(label_ids):\n",
    "                y_true_temp = []\n",
    "                y_pred_temp = []\n",
    "\n",
    "                for label_idx, label_id in enumerate(true_sentence_labels):\n",
    "\n",
    "                    # Skip 0 label\n",
    "                    if label_id == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Skip [CLS] label at sequence beginning\n",
    "                    if label_id == cls_token_id:\n",
    "                        continue\n",
    "\n",
    "                    # Detect [SEP] label at sentence end and ignore [SEP] and all sequence padding\n",
    "                    elif label_id == sep_token_id:\n",
    "                        y_true.append(y_true_temp)\n",
    "                        y_pred.append(y_pred_temp)\n",
    "                        break\n",
    "                    else:\n",
    "                        # Predicted NER label is special token: count preds\n",
    "                        if (logits[label_list_idx][label_idx] == 0):\n",
    "                            special_token_predictions = special_token_predictions +1\n",
    "                        # Predicted NER label is O: count preds\n",
    "                        elif (logits[label_list_idx][label_idx] == O_token_id):\n",
    "                            O_token_predictions = O_token_predictions +1\n",
    "                        else:\n",
    "                            ner_token_predictions = ner_token_predictions +1\n",
    "\n",
    "                        # Append label and prediction to list\n",
    "                        y_true_temp.append(label_map[label_id])\n",
    "                        y_pred_temp.append(label_map[logits[label_list_idx][label_idx]])\n",
    "\n",
    "        if (mode == 'Train' or mode == 'Validation'):\n",
    "            print(f\"True: {y_true[0:1]}, Predicted: {y_pred[0:1]}\")\n",
    "            report = classification_report(y_true, y_pred, digits=4, output_dict=True, zero_division = 0)\n",
    "            # Calculate epoch loss\n",
    "            epoch_loss = epoch_loss / len(data_loader)\n",
    "\n",
    "            # Print info to Tensorboard\n",
    "            writer.add_scalar(\"Loss\", epoch_loss, epoch)\n",
    "            macro_precision = report['macro avg']['precision']\n",
    "            writer.add_scalar(\"macro_avg/precision\", macro_precision, epoch)\n",
    "            macro_recall = report['macro avg']['recall']\n",
    "            writer.add_scalar(\"macro_avg/recall\", macro_recall, epoch)\n",
    "            macro_f1 = report['macro avg']['f1-score']\n",
    "            writer.add_scalar(\"macro_avg/f1\", macro_f1, epoch)\n",
    "\n",
    "            weighted_precision = report['weighted avg']['precision']\n",
    "            writer.add_scalar(\"weighted_avg/precision\", weighted_precision, epoch)\n",
    "            weighted_recall = report['weighted avg']['recall']\n",
    "            writer.add_scalar(\"weighted_avg/recall\", weighted_recall, epoch)\n",
    "            weighted_f1 = report['weighted avg']['f1-score']\n",
    "            writer.add_scalar(\"weighted_avg/f1\", weighted_f1, epoch)\n",
    "            print(f\"O Token Predictions: {O_token_predictions}, NER token predictions: {ner_token_predictions}\")\n",
    "            print(f\"loss: {epoch_loss} w prec: {weighted_precision} w recall: {weighted_recall} w f1: {weighted_f1}\")\n",
    "            return epoch_loss, macro_precision, macro_recall, macro_f1, weighted_precision, weighted_recall, weighted_f1\n",
    "\n",
    "        else:\n",
    "\n",
    "            report = classification_report(y_true, y_pred, digits=4, output_dict=False)\n",
    "            with open(results_dir +\"results.txt\", \"w\") as output:\n",
    "                print(\"***** Test results *****\")\n",
    "                print(f\"Task: {config.task}\")\n",
    "                print(f\"Model path: {config.saved_model_path}\")\n",
    "                print(f\"Data path: {config.data_path}\")\n",
    "                print(f\"Tokenizer: {config.tokenizer}\")\n",
    "                print(f\"Batch size: {config.batch_size}\")\n",
    "                print(f\"Epochs: {config.epochs}\")\n",
    "                print(f\"Learning rate: {config.learning_rate}\")\n",
    "                print(f\"Sequence length: {config.sequence_length}\")\n",
    "                print(f\"Training: {config.train_model}\")\n",
    "                print(f\"Num Threads: {config.num_threads}\")\n",
    "                print(f\"Num Sentences: {config.num_sentences}\")\n",
    "                print(f\"Max Grad Norm: {config.max_grad_norm}\")\n",
    "                print(f\"{report}\\n Special token predictions: {special_token_predictions}\")\n",
    "                output.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Loading Training Data\n",
      "Loading syntax graphs: from file en_gum-ud-train-bert-base-cased.syntree\n",
      "Example syntax graphs:\n",
      "[Data(x=[12, 768], edge_index=[2, 11], edge_attr=[11, 54]), Data(x=[9, 768], edge_index=[2, 8], edge_attr=[8, 54])]\n",
      "[{0: 1, 1: 10, 2: 11, 3: 2, 4: 7, 5: 8, 6: 9, 7: 4, 8: 5, 9: 3, 10: 6}, {0: 1, 1: 4, 2: 5, 3: 3, 4: 2, 5: 6, 6: 7, 7: 8}]\n",
      "Loading NER labels from ./data/ud/UD_English-GUM/**/*-train-orig.ner\n",
      "en_gum-ud-train-orig.ner\n",
      "10 sentences, 10 graphs, 5 batches of size 2\n",
      "Example of NER labels: [[['Aesthetic', 'O'], ['Appreciation', 'O'], ['and', 'O'], ['Spanish', 'S-NORP'], ['Art', 'O'], [':', 'O']], [['Insights', 'O'], ['from', 'O'], ['Eye-Tracking', 'O']]]\n",
      "['[CLS]', 'A', '##est', '##hetic', 'A', '##pp', '##re', '##ciation', 'and', 'Spanish', 'Art', ':', '[SEP]']\n",
      "['[CLS]', 'In', '##sight', '##s', 'from', 'Eye', '-', 'Track', '##ing', '[SEP]']\n",
      "Control example of InputFeatures\n",
      "Input Ids: [101, 1130, 18883, 1116, 1121, 9329, 118, 6563, 1158, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Input Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label Ids: [77, 1, 0, 0, 1, 1, 0, 0, 0, 78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Valid Ids: [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Label Mask: ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],)\n",
      "Segment Ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Syngnn dataset size: 10\n",
      "Loading Validation Data\n",
      "Loading syntax graphs: from file en_gum-ud-dev-bert-base-cased.syntree\n",
      "Example syntax graphs:\n",
      "[Data(x=[2, 768], edge_index=[2, 1], edge_attr=[1, 54]), Data(x=[41, 768], edge_index=[2, 40], edge_attr=[40, 54])]\n",
      "[{0: 1}, {0: 2, 1: 4, 2: 6, 3: 7, 4: 5, 5: 8, 6: 3, 7: 10, 8: 9, 9: 35, 10: 11, 11: 12, 12: 1, 13: 14, 14: 13, 15: 16, 16: 17, 17: 18, 18: 36, 19: 37, 20: 38, 21: 39, 22: 15, 23: 19, 24: 21, 25: 20, 26: 23, 27: 22, 28: 25, 29: 26, 30: 40, 31: 24, 32: 28, 33: 27, 34: 29, 35: 31, 36: 32, 37: 30, 38: 33, 39: 34}]\n",
      "Loading NER labels from ./data/ud/UD_English-GUM/**/*-dev-orig.ner\n",
      "en_gum-ud-dev-orig.ner\n",
      "10 sentences, 10 graphs, 5 batches of size 2\n",
      "Example of NER labels: [[['Introduction', 'O']], [['Research', 'O'], ['on', 'O'], ['adult-learned', 'O'], ['second', 'O'], ['language', 'O'], ['(', 'O'], ['L2', 'O'], [')', 'O'], ['has', 'O'], ['provided', 'O'], ['considerable', 'O'], ['insight', 'O'], ['into', 'O'], ['the', 'O'], ['neurocognitive', 'O'], ['mechanisms', 'O'], ['underlying', 'O'], ['the', 'O'], ['learning', 'O'], ['and', 'O'], ['processing', 'O'], ['of', 'O'], ['L2', 'O'], ['grammar', 'O'], ['[', 'O'], ['1', 'S-CARDINAL'], [']–[', 'O'], ['11', 'S-CARDINAL'], [']', 'O'], ['.', 'O']]]\n",
      "['[CLS]', 'Introduction', '[SEP]']\n",
      "['[CLS]', 'Research', 'on', 'adult', '-', 'learned', 'second', 'language', '(', 'L', '##2', ')', 'has', 'provided', 'considerable', 'insight', 'into', 'the', 'ne', '##uro', '##co', '##gni', '##tive', 'mechanisms', 'underlying', 'the', 'learning', 'and', 'processing', 'of', 'L', '##2', 'grammar', '[', '1', ']', '–', '[', '11', ']', '.', '[SEP]']\n",
      "Control example of InputFeatures\n",
      "Input Ids: [101, 2713, 1113, 4457, 118, 3560, 1248, 1846, 113, 149, 1477, 114, 1144, 2136, 5602, 14222, 1154, 1103, 24928, 11955, 2528, 22152, 3946, 10748, 10311, 1103, 3776, 1105, 6165, 1104, 149, 1477, 12616, 164, 122, 166, 782, 164, 1429, 166, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Input Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label Ids: [77, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 6, 1, 0, 0, 6, 1, 1, 78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Valid Ids: [1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Label Mask: ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],)\n",
      "Segment Ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Syngnn dataset size: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: 100%|██████████| 5/5 [00:10<00:00,  2.02s/it, loss=4.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Token Predictions: 20, NER token predictions: 88\n",
      "loss: 4.547056674957275 w prec: 0.0 w recall: 0.0 w f1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 0: 100%|██████████| 5/5 [00:05<00:00,  1.08s/it, loss=4.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Token Predictions: 0, NER token predictions: 286\n",
      "loss: 4.157225131988525 w prec: 0.0 w recall: 0.0 w f1: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if (config.train_model):\n",
    "    print(\"Training model\", flush=True)\n",
    "\n",
    "    trainLoader = loadTrainData()\n",
    "    validationLoader = loadValidationData()\n",
    "    epochs = config.epochs\n",
    "\n",
    "    train_writer = SummaryWriter(log_dir=tensorboard_dir+\"training\")\n",
    "    validation_writer = SummaryWriter(log_dir=tensorboard_dir+\"validation\")\n",
    "\n",
    "    epoch_losses_train = []\n",
    "    macro_precisions_train = []\n",
    "    macro_recalls_train = []\n",
    "    macro_f1s_train = []\n",
    "    weighted_precisions_train = []\n",
    "    weighted_recalls_train = []\n",
    "    weighted_f1s_train = []\n",
    "\n",
    "    epoch_losses_validation = []\n",
    "    macro_precisions_val = []\n",
    "    macro_recalls_val = []\n",
    "    macro_f1s_val = []\n",
    "    weighted_precisions_val = []\n",
    "    weighted_recalls_val = []\n",
    "    weighted_f1s_val = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #epoch_losses_train.append(trainModel(epoch, trainLoader, train_writer))\n",
    "        #epoch_losses_validation.append(validateModel(epoch, validationLoader, validation_writer))\n",
    "\n",
    "        epoch_loss_train, macro_precision_train, macro_recall_train, macro_f1_train, weighted_precision_train, weighted_recall_train, weighted_f1_train = runModelGNN(trainLoader, mode = 'Train', writer = train_writer, results_dir = results_dir, epoch=epoch)\n",
    "        epoch_losses_train.append(epoch_loss_train)\n",
    "        macro_precisions_train.append(macro_precision_train)\n",
    "        macro_recalls_train.append(macro_recall_train)\n",
    "        macro_f1s_train.append(macro_f1_train)\n",
    "\n",
    "        weighted_precisions_train.append(weighted_precision_train)\n",
    "        weighted_recalls_train.append(weighted_recall_train)\n",
    "        weighted_f1s_train.append(weighted_f1_train)\n",
    "\n",
    "        epoch_loss_val, macro_precision_val, macro_recall_val, macro_f1_val, weighted_precision_val, weighted_recall_val, weighted_f1_val = runModelGNN(validationLoader, mode = 'Validation', writer = validation_writer, results_dir = results_dir, epoch=epoch)\n",
    "        epoch_losses_validation.append(epoch_loss_val)\n",
    "        macro_precisions_val.append(macro_precision_val)\n",
    "        macro_recalls_val.append(macro_recall_val)\n",
    "        macro_f1s_val.append(macro_f1_val)\n",
    "\n",
    "        weighted_precisions_val.append(weighted_precision_val)\n",
    "        weighted_recalls_val.append(weighted_recall_val)\n",
    "        weighted_f1s_val.append(weighted_f1_val)\n",
    "\n",
    "\n",
    "        if (activeMode == 'prod'):\n",
    "            # Save model after each epoch\n",
    "            model.save_pretrained(save_directory=trained_models_dir)\n",
    "    \n",
    "    train_writer.close()\n",
    "    validation_writer.close()\n",
    "    prof.stop()\n",
    "    # Save epoch loss plots\n",
    "    plt.figure()\n",
    "    plt.plot(range(0,epochs), epoch_losses_train, 'b', label='Training')\n",
    "    plt.plot(range(0,epochs), epoch_losses_validation, 'g', label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(results_dir +\"/loss.png\", facecolor='white', transparent=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # # Save macro f1 plot\n",
    "    plt.figure()\n",
    "    plt.plot(range(0,epochs), macro_f1s_train, 'b', label='Training')\n",
    "    plt.plot(range(0,epochs), macro_f1s_val, 'g', label='Validation')\n",
    "    plt.title('Macro Avg F1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1')\n",
    "    plt.legend()\n",
    "    plt.savefig(results_dir +\"/f1_macro.png\", facecolor='white', transparent=False)\n",
    "    # #plt.show()\n",
    "\n",
    "    # # Save weighted f1 plot\n",
    "    plt.figure()\n",
    "    plt.plot(range(0,epochs), weighted_f1s_train, 'b', label='Training')\n",
    "    plt.plot(range(0,epochs), weighted_f1s_val, 'g', label='Validation')\n",
    "    plt.title('Weighted Avg F1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1')\n",
    "    plt.legend()\n",
    "    plt.savefig(results_dir +\"/f1_weighted.png\", facecolor='white', transparent=False)\n",
    "    # #plt.show()\n",
    "\n",
    "    # # Save weighted recall plot\n",
    "    plt.figure()\n",
    "    plt.plot(range(0,epochs,1), weighted_recalls_train, 'b', label='Training')\n",
    "    plt.plot(range(0,epochs,1), weighted_recalls_val, 'g', label='Validation')\n",
    "    plt.title('Weighted Avg Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    plt.savefig(results_dir +\"/recall_weighted.png\", facecolor='white', transparent=False)\n",
    "    # #plt.show()\n",
    "\n",
    "\n",
    "    # # Save macro recall plot\n",
    "    plt.figure()\n",
    "    plt.plot(range(0,epochs,1), macro_recalls_train, 'b', label='Training')\n",
    "    plt.plot(range(0,epochs,1), macro_recalls_val, 'g', label='Validation')\n",
    "    plt.title('Macro Avg Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    plt.savefig(results_dir +\"/recall_macro.png\", facecolor='white', transparent=False)\n",
    "    # #plt.show()\n",
    "\n",
    "    #  # Save weighted precision plot\n",
    "    plt.figure()\n",
    "    plt.plot(range(0,epochs,1), weighted_precisions_train, 'b', label='Training')\n",
    "    plt.plot(range(0,epochs,1), weighted_precisions_val, 'g', label='Validation')\n",
    "    plt.title('Weighted Avg Precision')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    plt.savefig(results_dir +\"/precision_weighted.png\", facecolor='white', transparent=False)\n",
    "    # #plt.show()\n",
    "\n",
    "    # # Save macro precision plot\n",
    "    plt.figure()\n",
    "    plt.plot(range(0,epochs,1), macro_precisions_train, 'b', label='Training')\n",
    "    plt.plot(range(0,epochs,1), macro_precisions_val, 'g', label='Validation')\n",
    "    plt.title('Macro Avg Precison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    plt.savefig(results_dir +\"/precision_macro.png\", facecolor='white', transparent=False)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model evaluation\\n\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loader = loadTestData()\n",
    "#runModelGNN(test_loader, mode = 'Test', results_dir=results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_matrix = metrics.confusion_matrix(references_all, predictions_all, labels=labels)\n",
    "#print(confusion_matrix)\n",
    "#disp = metrics.ConfusionMatrixDisplay(references_all, predictions_all, labels=labels)\n",
    "#disp.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv_syntrans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e86a731642ee256d624a4d29e8688bb9c6ad7b39856affb444c6cc9f38126795"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
